{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install some packages\n",
    "\n",
    "packages installed from within notebook seems to not persist when server shutdown\n",
    "\n",
    "conda install was too slow solving environment, didn't bother"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (2.2.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from spacy) (2.20.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from spacy) (2.0.3)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from spacy) (1.0.2)\n",
      "Requirement already satisfied: thinc<7.4.0,>=7.3.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from spacy) (7.3.1)\n",
      "Requirement already satisfied: setuptools in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from spacy) (39.1.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from spacy) (1.17.4)\n",
      "Requirement already satisfied: srsly<1.1.0,>=0.1.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from spacy) (0.2.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from spacy) (3.0.2)\n",
      "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from spacy) (0.4.0)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from spacy) (0.0.8)\n",
      "Requirement already satisfied: idna<2.8,>=2.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.6)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.23)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2019.9.11)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from thinc<7.4.0,>=7.3.0->spacy) (4.39.0)\n",
      "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from catalogue<1.1.0,>=0.0.7->spacy) (0.23)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy) (0.6.0)\n",
      "Requirement already satisfied: more-itertools in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from zipp>=0.5->importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy) (4.1.0)\n",
      "Requirement already satisfied: six<2.0.0,>=1.0.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from more-itertools->zipp>=0.5->importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy) (1.11.0)\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 19.3.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "Requirement already satisfied: en_core_web_sm==2.2.5 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz#egg=en_core_web_sm==2.2.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (2.2.5)\n",
      "Requirement already satisfied: spacy>=2.2.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from en_core_web_sm==2.2.5) (2.2.3)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.2)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.2)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.17.4)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.0.8)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.20.0)\n",
      "Requirement already satisfied: thinc<7.4.0,>=7.3.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.3.1)\n",
      "Requirement already satisfied: setuptools in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (39.1.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.3)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n",
      "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
      "Requirement already satisfied: srsly<1.1.0,>=0.1.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.2.0)\n",
      "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (0.23)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.23)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2019.9.11)\n",
      "Requirement already satisfied: idna<2.8,>=2.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.6)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from thinc<7.4.0,>=7.3.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (4.39.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (0.6.0)\n",
      "Requirement already satisfied: more-itertools in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from zipp>=0.5->importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (4.1.0)\n",
      "Requirement already satisfied: six<2.0.0,>=1.0.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from more-itertools->zipp>=0.5->importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.11.0)\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 19.3.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2mâœ” Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('en_core_web_sm')\n",
      "Requirement already satisfied: wordcloud in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (1.6.0)\n",
      "Requirement already satisfied: pillow in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from wordcloud) (5.2.0)\n",
      "Requirement already satisfied: numpy>=1.6.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from wordcloud) (1.17.4)\n",
      "Requirement already satisfied: matplotlib in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from wordcloud) (3.0.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from matplotlib->wordcloud) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from matplotlib->wordcloud) (1.0.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from matplotlib->wordcloud) (2.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from matplotlib->wordcloud) (2.7.3)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from cycler>=0.10->matplotlib->wordcloud) (1.11.0)\n",
      "Requirement already satisfied: setuptools in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from kiwisolver>=1.0.1->matplotlib->wordcloud) (39.1.0)\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 19.3.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'nltk' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-ffb6bd844e32>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' python -m spacy download en_core_web_sm'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' pip install wordcloud'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'brown'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'nltk' is not defined"
     ]
    }
   ],
   "source": [
    "! pip install spacy\n",
    "! python -m spacy download en_core_web_sm\n",
    "! pip install wordcloud\n",
    "nltk.download('brown')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Just some imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import re\n",
    "from pprint import pprint\n",
    "import nltk\n",
    "from nltk.corpus import words, brown\n",
    "import spacy\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "import boto3 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All functions are here\n",
    "\n",
    "Not the best readiblity for notebook, since you have to jump back and forth to read function definitions\n",
    "\n",
    "But it's the best for clarity and later refactoring/producitonising"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def re_find_all_start_positions(regex_pat: str, text: str) -> list:\n",
    "    matches = list( re.finditer(regex_pat, text, flags=re.MULTILINE) )\n",
    "    start_positions = [m.start() for m in matches]\n",
    "    return start_positions\n",
    "    \n",
    "def re_split(regex_pat: str, text: str) -> list:\n",
    "    \"\"\"Custom regex split solution, re.split() either throw away the delimiter \n",
    "    or keep it as separate items, doesn't suit the needs here.\n",
    "    \"\"\"\n",
    "    split_positions = re_find_all_start_positions(regex_pat, text)\n",
    "    if split_positions[0] > 0:  # add starting position if it's not there\n",
    "        split_positions = [0] + split_positions\n",
    "    split_positions = split_positions + [len(text)]  # add end position\n",
    "\n",
    "    splits = []\n",
    "    n_splits = len(split_positions) - 1\n",
    "    for i in range(n_splits):\n",
    "        splits.append(text[split_positions[i] : split_positions[i+1]])\n",
    "    \n",
    "    assert(''.join(splits) == text)\n",
    "    \n",
    "    return splits\n",
    "\n",
    "def strip_scene_header(scene: str) -> str:\n",
    "    return re.sub(r\"^\\tHAMLET\\n+ACT [A-Z]+\\n+SCENE [A-Z]+\\t.+\\n+\", \"\", scene, flags=re.MULTILINE)\n",
    "\n",
    "def re_search(pat: str, text: str) -> str:\n",
    "    \"\"\"re.search() wrapper\n",
    "    returns first matching group, with MULTILINE flag\n",
    "    returns N/A if not found\"\"\"\n",
    "    match = re.search(pat, text, flags=re.MULTILINE)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    else:\n",
    "        return \"N/A\"\n",
    "\n",
    "def get_act_name(scene_header: str) -> str:\n",
    "    return re_search(r\"^\\tHAMLET\\n+(ACT [A-Z]+)(\\n+)\", scene_header)\n",
    "    \n",
    "def get_scene_name(scene_header: str) -> str:\n",
    "    return re_search(r\"(^SCENE [A-Z]+)(?:\\t)\", scene_header)\n",
    "    \n",
    "def get_scene_title(scene_header: str) -> str:\n",
    "    return re_search(r\"(?:^SCENE [A-Z]+\\t)(.+$)\", scene_header)\n",
    "    \n",
    "def get_scenes_df(full_text: str) -> pd.DataFrame:\n",
    "    scenes = re_split(r\"^\\tHAMLET\\n+ACT [A-Z]+\\n+SCENE [A-Z]+\\t.+\\n+\", full_text)\n",
    "    \n",
    "    scenes_dict_list = []\n",
    "    for s in scenes:\n",
    "        act_name = get_act_name(s)\n",
    "        scene_name = get_scene_name(s)\n",
    "        scene_title = get_scene_title(s)\n",
    "        scene_text = s\n",
    "        scenes_dict_list.append({'act_name': act_name, 'scene_name': scene_name, \n",
    "                                'scene_title': scene_title, 'scene_text': scene_text})\n",
    "    \n",
    "    scenes_df = pd.DataFrame(scenes_dict_list)\n",
    "    assert(''.join(scenes_df.scene_text) == full_text)\n",
    "    return scenes_df\n",
    "\n",
    "def get_scene_header(scene: str) -> str:\n",
    "    return re_search(r\"(^\\tHAMLET\\n+ACT [A-Z]+\\n+SCENE [A-Z]+\\t.+\\n+)\", scene)\n",
    "\n",
    "def split_scene_into_paragraphs(scene_df_row: pd.Series) -> pd.DataFrame:\n",
    "    scene = scene_df_row.scene_text\n",
    "    scene_header = get_scene_header(scene)\n",
    "    \n",
    "    paragraphs = []    \n",
    "    if scene_df_row.scene_name == 'N/A':  # cast section at the beginning of the book\n",
    "        paragraphs.append({'paragraph_type': 'cast', 'paragraph_text': scene})\n",
    "    else:  # a proper scene\n",
    "        scene_body = strip_scene_header(scene)\n",
    "        header_paragraph = {'paragraph_type': 'header', 'paragraph_text': scene_header}\n",
    "        paragraphs.append(header_paragraph)\n",
    "        \n",
    "        re_speech_header = r\"(?<=\\n\\n)[A-Z][A-Za-z ]+\\t\"\n",
    "        re_action_paragraph = r\"^\\s*\\[.+$\"\n",
    "\n",
    "        for paragraph_text in re_split(re_speech_header, scene_body):\n",
    "            if re.match(re_action_paragraph, paragraph_text, flags=re.MULTILINE):\n",
    "                paragraph_type = 'action'\n",
    "            else:\n",
    "                paragraph_type = 'speech'\n",
    "            paragraph_dict = {'paragraph_type': paragraph_type, 'paragraph_text': paragraph_text}\n",
    "            paragraphs.append(paragraph_dict)\n",
    "    \n",
    "    # convert to dataframe and copy act/scene info from scene dataframe\n",
    "    paragraphs_df = pd.DataFrame(paragraphs)\n",
    "    scene_info = scene_df_row.drop('scene_text')\n",
    "    for col, val in scene_info.iteritems():\n",
    "        paragraphs_df[col] = val\n",
    "    \n",
    "    assert( ''.join(paragraphs_df.paragraph_text) == scene_df_row.scene_text )\n",
    "    return paragraphs_df\n",
    "\n",
    "def get_paragraphs_df(scenes_df: pd.DataFrame) -> pd.DataFrame: \n",
    "    paragraphs_df = pd.concat([ split_scene_into_paragraphs(row) for i, row in scenes_df.iterrows() ])\n",
    "    paragraphs_df = paragraphs_df.reset_index(drop=True)  # reset index to [0,1,2,3,....]\n",
    "    paragraphs_df = paragraphs_df.reset_index().rename(columns={'index': 'paragraph_id'})  # convert index to paragraph_id\n",
    "    paragraphs_df['characters'] = paragraphs_df.paragraph_text.map(get_characters_in_paragraph)\n",
    "    return paragraphs_df\n",
    "\n",
    "def get_characters_in_paragraph(paragraph_text: str) -> list:\n",
    "    chars = re.findall(r\"(?!^SCENE.+)(^[A-Z][A-Za-z ]+)(?:\\t)\", paragraph_text, flags=re.MULTILINE)\n",
    "    return chars\n",
    "\n",
    "def split_paragraph_into_lines(paragraph_df_row: pd.Series) -> pd.DataFrame:\n",
    "    paragraph_text = paragraph_df_row.paragraph_text\n",
    "    lines = re_split(r\"^.*\\n\", paragraph_text)\n",
    "    \n",
    "    re_empty_line = r\"(^[A-Z][A-Za-z ]+\\t[\\|\\s]*$)|(^[\\s]*$)\"  # also match 'CHARACTER    |' as empty line\n",
    "    re_character_name = r\"^[A-Z][A-Za-z ]+\\t\"\n",
    "    re_action_line = r\"^\\s*\\[.*\\]\\s*$\"\n",
    "    \n",
    "    lines_dict_list = []\n",
    "    for line in lines:\n",
    "        if re.match(re_empty_line, line, flags=re.MULTILINE):\n",
    "            line_type = 'empty'\n",
    "        elif paragraph_df_row.paragraph_type == 'cast':\n",
    "            line_type = 'cast'\n",
    "        elif paragraph_df_row.paragraph_type == 'header':\n",
    "            line_type = 'header'\n",
    "        elif paragraph_df_row.paragraph_type == 'action':\n",
    "            line_type = 'action'\n",
    "        elif re.match(re_action_line, line, flags=re.MULTILINE):\n",
    "            line_type = 'action'\n",
    "        else:\n",
    "            line_type = 'speech'\n",
    "        line_dict = {'line_type': line_type, 'line': line}\n",
    "        lines_dict_list.append(line_dict)\n",
    "    lines_df = pd.DataFrame(lines_dict_list)\n",
    "    # copy paragraph info over except paragraph_text\n",
    "    paragraph_info = paragraph_df_row.drop('paragraph_text')\n",
    "    for col, val in paragraph_info.iteritems():\n",
    "        lines_df[col] = [val] * len(lines_df)  # had to use this special pattern since character column contains list\n",
    "    \n",
    "    assert(''.join(lines_df.line) == paragraph_df_row.paragraph_text)\n",
    "    return lines_df\n",
    "\n",
    "def get_lines_df(paragraphs_df: pd.DataFrame) -> pd.DataFrame: \n",
    "    lines_df = pd.concat([ split_paragraph_into_lines(row) for i, row in paragraphs_df.iterrows() ])\n",
    "    lines_df = lines_df.reset_index(drop=True)  # reset index to [0,1,2,3,....]\n",
    "    lines_df = lines_df.reset_index().rename(columns={'index': 'line_id'})  # convert index to line_id\n",
    "    return lines_df\n",
    "\n",
    "def spacy_lemmatise(text: str) -> list:\n",
    "    # tried to lemmatise the tokens to use it with a smaller corpus (nltk's words.words())\n",
    "    # turned out it's better to simply use a bigger corpus\n",
    "    nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "    doc = nlp(text)\n",
    "    lemmas = [token.lemma_.lower() for token in doc if token.is_alpha]\n",
    "    tokens = [token.text.lower() for token in doc if token.is_alpha]\n",
    "    context = []\n",
    "    for i in range(len(tokens)):\n",
    "        i_start = i-5 if i>=5 else 0\n",
    "        i_end = i+5 if i<=len(tokens)-5 else len(tokens)\n",
    "        context.append(' '.join([t for t in tokens[i_start:i_end]]))\n",
    "    lemma_map = pd.DataFrame({'token': tokens, 'lemma': lemmas, 'context': context}).drop_duplicates()\n",
    "    return tokens, lemmas, lemma_map\n",
    "\n",
    "def get_common_english_words(topn: int) -> list:\n",
    "    return pd.Series([w.lower() for w in brown.words()])\\\n",
    "            [lambda s: s.str.isalpha()]\\\n",
    "            .value_counts().head(topn).index.values.tolist()\n",
    "\n",
    "def show_word_cloud(text: str):\n",
    "    wordcloud = WordCloud(width=800, height=400).generate(text)\n",
    "    fig, ax = plt.subplots(figsize=(15,8))\n",
    "    _ = plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    _ = plt.axis(\"off\")\n",
    "    \n",
    "def detect_sentiment_batch(texts: list) -> tuple:\n",
    "    comprehend = boto3.client('comprehend', region_name='ap-southeast-2')\n",
    "    results = comprehend.batch_detect_sentiment(TextList=texts, LanguageCode='en')\n",
    "    sentiments = [ r['Sentiment'] for r in results['ResultList'] ]\n",
    "    positive_scores = [ r['SentimentScore']['Positive'] for r in results['ResultList'] ]\n",
    "    negative_scores = [ r['SentimentScore']['Negative'] for r in results['ResultList'] ]\n",
    "    neutral_scores = [ r['SentimentScore']['Neutral'] for r in results['ResultList'] ]\n",
    "    mixed_scores = [ r['SentimentScore']['Mixed'] for r in results['ResultList'] ]\n",
    "    return sentiments, positive_scores, negative_scores, neutral_scores, mixed_scores\n",
    "\n",
    "def batch_sentiment(texts: list) -> list:\n",
    "    max_batch_size = 25  # Comprehend API batch size limit\n",
    "    sentiments = []\n",
    "    for i in range(0, len(texts), max_batch_size):\n",
    "        batch_texts = texts[i: i + max_batch_size]\n",
    "        # made it return the individual scores but didn't end up using them\n",
    "        batch_sentiments, _, _, _, _ = detect_sentiment_batch(batch_texts)\n",
    "        sentiments = sentiments + batch_sentiments\n",
    "        print(f'Processed {len(sentiments)} texts')\n",
    "    return sentiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters\n",
    "\n",
    "Thought there would be more but ended up a single one...\n",
    "\n",
    "Some of the stuff should be paramters though if we want to reuse/productionise like the regex patterns\n",
    "\n",
    "They were actually in here before I decided to move them into functions for better readability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FILE = \"hamlet.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"main\" routines starts here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_book = open(DATA_FILE).read()\n",
    "\n",
    "full_book[:300]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert into Scenes dataframe\n",
    "\n",
    "Each Scene would be a row, and I decided to do a loss-less split, in a sense that if you simply concatenate all scene_text, you will end up getting the exact document before split. \n",
    "\n",
    "This was inspired by spacy's non-destructive tokenisation, I always admired their approach and this is an opportunity to do something similar. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "scenes_df = get_scenes_df(full_book)\n",
    "\n",
    "scenes_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further split into paragraphs\n",
    "\n",
    "Again the splits are loss less"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraphs_df = get_paragraphs_df(scenes_df)\n",
    "\n",
    "paragraphs_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split into lines\n",
    "\n",
    "The document eventually became a denormalised data table (OLAP's favourite :) )\n",
    "\n",
    "Note each line is tagged as header/speech/action/empty etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines_df = get_lines_df(paragraphs_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines_df.iloc[410:420, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify that it's still a loss-less split end to end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''.join(lines_df.line) == full_book"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get character lines\n",
    "\n",
    "One of the tricky things was multiple characters may say the same line together, it's taken care of during the paragraph parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "speech_lines = lines_df[lambda df: df.line_type=='speech'][['characters', 'line', 'line_id']]\n",
    "\n",
    "character_lines = pd.DataFrame()\n",
    "for i, line in speech_lines.iterrows():\n",
    "    for char in line.characters:\n",
    "        row = line.to_dict()\n",
    "        row.pop('characters')\n",
    "        row['character'] = char\n",
    "        character_lines = character_lines.append(row, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "character_lines.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number of lines per character\n",
    "\n",
    "With all the hard work above, this is now very simple. \n",
    "\n",
    "This version seems to have fewer lines comparing to the numbers listed on Wikipedia?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "character_lines.groupby('character').size().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get all the non empty lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_empty_lines = ''.join(lines_df[lambda df: df.line_type!='empty'].line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use spacy for tokenising\n",
    "\n",
    "Tried lemmatising, turned out it wasn't necessary with a bigger dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens, tokens_lemmatised, lemma_map = spacy_lemmatise(non_empty_lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get a list of common enligsh words\n",
    "nltk's Brown corpus was created in 1961, seems to be a good choice for older literature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_english_words = get_common_english_words(10000)\n",
    "\n",
    "common_english_words[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get a list of character names\n",
    "\n",
    "to be excluded in the word cloud\n",
    "\n",
    "some characters were referred to without titles, hence the added list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_names = character_lines.character.unique().tolist() + ['CLAUDIUS', 'POLONIUS', 'GERTRUDE', 'FORTINBRAS']\n",
    "char_names = [c.lower() for c in char_names]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here are the unusal tokens (words)\n",
    "\n",
    "Convert to Series since list comprehension was too slow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_unusual = pd.Series(tokens)\\\n",
    "                [lambda s: ~s.isin(common_english_words + char_names)]  \n",
    "\n",
    "tokens_unusual.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unusual word cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_word_cloud(' '.join(tokens_unusual))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get sentiments using AWS Comprehend\n",
    "\n",
    "Using a top-tier paid NLP API seems cheating, but realistically the next best free option is probably fine-tuning BERT with a public dataset. But tuning BERT itself is costly and a good public sentiment dataset is hard to find (especially for old english literature). The only off the shelf options that can be done within a few hours are probably nltk and TextBlob. However they are mostly rule based, and work even worse with longer text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AWS Comprehend's sentiment API has a limit of 5KB per document\n",
    "paragraphs_df.paragraph_text.map(len).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scene_paragraphs = paragraphs_df[lambda df: df.act_name!='N/A'].copy() # exclude the cast section at the start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sentiments = batch_sentiment(scene_paragraphs.paragraph_text.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scene_paragraphs['sentiment'] = sentiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number of paragraphs by sentiment type per scene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_by_scene = scene_paragraphs.\\\n",
    "                        pivot_table(index=['act_name', 'scene_name'], columns='sentiment', aggfunc='size')\\\n",
    "                        .fillna(0)\n",
    "\n",
    "sentiment_by_scene"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment Ratios per Scene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_by_scene.pipe(lambda df: df.divide(df.sum(axis=1), axis=0) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment per Scene Visualised\n",
    "\n",
    "The last scene seems pretty tragic.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_by_scene.pipe(lambda df: df.divide(df.sum(axis=1), axis=0) )\\\n",
    ".plot.bar(stacked=True, figsize=(14,7))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
